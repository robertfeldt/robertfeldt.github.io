# Bayesian Data Analysis for Software Engineering

Some pointers to help you get started doing Bayesian analysis on your SE data.

## ICSE 2021 Technical Briefing

We (Richard Torkar, Carlo Furia and yours truly) held a technical briefing (short tutorial) on Bayesian Data Analysis for SE at ICSE 2021 ("in" Madrid, i.e. online/virtual).

- [YouTube playlist](http://tiny.cc/bayes-icse21) with videos (3 parts but split into 4 videos)
  1. [Ten reasons for using Bayesian data analysis in empirical software engineering](https://youtu.be/Qf7oNkZan3U)
  2. How do we do Bayesian data analysis?
    - a, [An overview](https://youtu.be/4z-8-4BSl1M)
    - b, [A small hands-on example](https://youtu.be/U6s5-fGPoxg) [Code/scripts](https://github.com/torkar/icse_tutorial)
  3. [Using baysian data analysis in software engineering](https://youtu.be/MVxGYezzn9s) [Slides](UsingBDAinSE_part3_ICSE2021_TechBriefing_Feldt.pdf)

## FAQ

### Q1. BDA sounds great, but how do **I** get started?

We recommend you buy and read/follow the book [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath. We **strongly** recommend you get the 2nd edition since it involves and is based on causal analysis which will be of great importance for science (and SE) longer-term.

### Q2. What if I want a more SE-specific starting point?

You might find it useful to start with our "trifecta" of papers arguing for and providing SE-specific processes and examples:

1. Furia, C. A., R. Feldt, and R. Torkar. "[Bayesian data analysis in empirical software engineering research](https://arxiv.org/pdf/1811.05422.pdf)." IEEE Transactions on Software Engineering (2019). [IEEE link](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807222&casa_token=RtQXHSjHI50AAAAA:U8nb4QzGFyfI4Pb0-246vfowyUhFVSgLsdjLYO44rgUTRDGwma3XfCaOI-i8LOajqCkoi7sG&tag=1)
    - Summarizes some disadvantages of traditional, frequentist statistics
    - Argues that Bayesian statistical analysis should have a more prominent role in SE
    - High-level overview of Bayesian statists
    - Re-analyses two SE datasets
2. Torkar, R., C. A. Furia, R. Feldt, ... "[A Method to Assess and Argue for Practical Significance in Software Engineering](https://arxiv.org/pdf/1809.09849.pdf)." IEEE Transactions on Software Engineering (2020). [IEEE link](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9314270)
3. Furia, C. A., R. Torkar, and R. Feldt. "[Applying Bayesian Analysis Guidelines to Empirical Software Engineering Data: The Case of Programming Languages and Code Quality](https://arxiv.org/pdf/2101.12591.pdf).", 2021, in submission.

If you find them useful and then go on to publish and use BDA in your paper we would appreciate if you cite our work.

### Q3. Where can I find examples of BDA on SE data sets?

Our published papers arguing for BDA have analyzed a few different SE-related data sets:

1. Effectiveness of autogenerated vs manual testing: Section 3.3 of [paper 1](https://arxiv.org/pdf/1809.09849.pdf) analyses data from the paper [Ceccato2015](https://dl.acm.org/doi/pdf/10.1145/2768829), "Do Automatically Generated Test Cases Make Debugging Easier? An Experimental Assessment of Debugging Effectiveness and Efficiency".
2. Run time performance of programming languages: Section 4.1 of [paper 1](https://arxiv.org/pdf/1809.09849.pdf) analyses data from the paper [Nanz2015](https://ieeexplore.ieee.org/iel7/7174815/7194545/07194625.pdf), "A Comparative Study of Programming Languages in Rosetta Code".
3. Effectiveness of exploratory vs scripted testing: Section 3 of [paper 2](https://arxiv.org/pdf/1809.09849.pdf) analyses data from the paper [Afzal2015](https://www.diva-portal.org/smash/get/diva2:834260/FULLTEXT02), "An experiment on the effectiveness and efficiency of exploratory testing".

Our paper that is currently in submission analyzes:

4. Programming language and code quality: Section 3 of [paper 3](https://arxiv.org/pdf/2101.12591.pdf) analyses data from the paper [Ray2014](https://dl.acm.org/doi/pdf/10.1145/2635868.2635922), "A large scale study of programming languages and code quality in Github".

Other papers in SE include:

5. Setting software metrics tresholds: [Ernst2018](https://dl.acm.org/doi/pdf/10.1145/3196398.3196443), "Bayesian Hierarchical Modelling for Tailoring Metric Thresholds", MSR 2018.
6. Affective states and technical debt: [Olsson2020](https://arxiv.org/pdf/2009.10660.pdf), "Measuring affective states from technical debt: A psychoempirical software engineering experiment", accepted for publication in EMSE journal.
7. Fault localization algorithm: [Scholz2020](https://arxiv.org/pdf/2007.09394.pdf), "An empirical study of Linespots: A novel past-fault algorithm", in submission.
8. Requirements prioritization criteria: [BerntssonSvensson2021](https://arxiv.org/pdf/2104.06033.pdf), "Not all requirements prioritization criteria are equal at all times: A quantitative analysis", in submission.

Most likely there are also other and earlier examples and we would like to collect them. If you know of papers that do Bayesian analysis of SE-related data please contact us or make a pull request to this page. Thanks!

### Q4. Which tools do you recommend?

Find an up-to-date library that is easy for you to work with in a language and with tools you already know. The main workhorse of modern BDA is the [Stan tool](https://mc-stan.org) but it is easier to use it from libraries in your language of choice:

1. [brms](https://github.com/paul-buerkner/brms) for R
2. [Stan.jl](https://github.com/StanJulia/Stan.jl) for Julia
3. [PyStan](https://pystan.readthedocs.io/en/latest/) for Python
4. There are also Stan interfaces for Matlab, Stata, Mathematica etc, see [Stan interfaces](https://mc-stan.org/users/interfaces/)

For the future we are optimistic about the [Turing.jl](https://turing.ml) library for Julia since it has the potential to be even more flexible, powerful, and scalable/fast than solutions based on Stan. However, it is not yet as mature as libraries and tools based on Stan.

### Q5. Are there guidelines and recommended workflows for doing BDA in SE?

Workflows and guidelines are currently under development in statistics and there is not yet a clear consensus. For a detailed and up-to-date guide see the current version of the ["Bayesian Workflow"](https://arxiv.org/pdf/2011.01808.pdf) book being written by Gelman et al. We have tried to present a shorter, condensed workflow for BDA in SE in Section 2 of [paper 3](https://arxiv.org/pdf/2101.12591.pdf).

### Q6. Is it true that statisticians discourages the use of p-values?

The ASA ([American Statistical Association](https://www.amstat.org)) in their 2016 ["ASA Statement on P-Values"](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108) discouraged the use of declarations of "statistical significance". In a 2019 editorial of the ASA journal AST the Editors of the special issue on ["Moving to a world beyond p < 0.05"](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913) then said: 

"We conclude, based on our review of the articles in this special issue and the broader literature, that **it is time to stop using the term 'statistically significant' entirely**. Nor should variants such as 'statistically different', 'p < 0.05' and 'nonsignificant' survive, whether expressed in words, by asterisk in a table, or in some other way". 

See [slides 20 and 21](https://speakerdeck.com/robertfeldt/empirical-software-engineering-as-a-science-challenges-and-ways-forward?slide=20) in Robert Feldt's ESEM 2019 keynote for links and the actual quote. For what we should do instead see [Slide 23](https://speakerdeck.com/robertfeldt/empirical-software-engineering-as-a-science-challenges-and-ways-forward?slide=23).